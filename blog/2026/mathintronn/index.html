<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Mathematical Introduction to Neural Networks | Agastya Agrawal </title> <meta name="author" content="Agastya Agrawal"> <meta name="description" content="I am currently a second year undergraduate at Chennai Mathematical Institute pursuing my Bachelors in Mathematics and Computer Science. "> <meta name="keywords" content="ai-alignment, mechanistic-interpretability, machine-learning, reinforcement-learning, cmi"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A7%A0&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://agastyaagrawal.github.io/blog/2026/mathintronn/"> <script src="/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Agastya</span> Agrawal </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Home </a> </li> <li class="nav-item "> <a class="nav-link" href="/about-me/">About Me </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/research/">Research </a> </li> <li class="nav-item "> <a class="nav-link" href="/talks/">Talks </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Mathematical Introduction to Neural Networks</h1> <p class="post-meta"> Created on February 10, 2026 </p> <p class="post-tags"> <a href="/blog/2026"> <i class="fa-solid fa-calendar fa-sm"></i> 2026 </a>   ·   <a href="/blog/category/machine-learning"> <i class="fa-solid fa-tag fa-sm"></i> Machine Learning</a>   <a href="/blog/category/ai-alignment"> <i class="fa-solid fa-tag fa-sm"></i> AI Alignment</a>   <a href="/blog/category/slt"> <i class="fa-solid fa-tag fa-sm"></i> SLT</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="neural-networks-and-the-bayesian-posterior">Neural Networks and the Bayesian Posterior</h1> <p><strong>Aim:</strong> Using a concrete example of Neural Networks (NNs), introduce the posterior and the free energy, thus explaining the intuition of:</p> \[F_n \approx n \min_{\omega \in W} L_n(\omega) + \lambda \log n\] <blockquote> <p><strong>Key Insight:</strong> In singular models, non-true parameters can nonetheless be preferred by the posterior.</p> </blockquote> <hr> <h2 id="feedforward-relu-neural-networks">Feedforward ReLU Neural Networks</h2> <p><strong>Definition:</strong> A feedforward ReLU Neural Net function is a function: \(f : \mathbb{R}^N \times W \longrightarrow \mathbb{R}^H\)</p> <ul> <li>$N$: number of inputs (in statistical models)</li> <li>$W$: parameter space</li> <li>$H$: number of outputs</li> </ul> <h3 id="architecture-diagram">Architecture Diagram</h3> <pre><code class="language-mermaid">graph LR
    subgraph Inputs
    x1((x1))
    dots1[...]
    xn((xn))
    end
    
    subgraph HiddenLayer1 [Hidden Layer 1]
    h1_1(( ))
    h1_2(( ))
    dots2[...]
    h1_n(( ))
    end

    subgraph HiddenLayerL [Hidden Layer L-1]
    hL_1(( ))
    hL_2(( ))
    end

    subgraph Output
    y1((y1))
    dots3[...]
    yn((yn))
    end

    x1 --- h1_1
    x1 --- h1_2
    xn --- h1_1
    xn --- h1_n
    h1_1 --- hL_1
    h1_n --- hL_2
    hL_1 --- y1
    hL_2 --- yn
    hL_2 --- y1
</code></pre> <p>This induces a function where both $x$ and $w$ are vectors: \(f(x, w) = (A^L \circ \text{ReLU} \circ A^{L-1} \circ \text{ReLU} \dots \circ A^1)(x)\)</p> <p>Each $A^L$ is parameterized by weights $W^L$ and biases $b^L$: \(A^L(z) = (w^L)^T z + b^L\)</p> <p><em>Note: You do not evaluate with ReLU on the last layer in regression.</em></p> <hr> <h2 id="regression-with-neural-networks">Regression with Neural Networks</h2> <table> <tbody> <tr> <td>We define the triplet $(\text{model, truth, prior}) = (P(y</td> <td>x,w), q(y</td> <td>x), \varphi(w))$.</td> </tr> </tbody> </table> <h3 id="1-truth">1. Truth</h3> <p>We suppose i.i.d. data $d_n = { (X_1, Y_1), \dots, (X_n, Y_n) }$ which is drawn from an unknown true distribution $q(y|x)$.</p> <h3 id="2-model">2. Model</h3> <p>Given a FFNN function $f(x,w)$, the regression model is given by: \(p(y|x,w) = f(x,w) + \varepsilon, \quad \varepsilon \sim N(0,1) \text{ (noise)}\) \(p(y|x,w) = \exp \left( -\frac{1}{2} \| y - f(x,w) \|^2_W \right)\)</p> <p>It is the property of $f(x,w)$ that determines whether the model is <strong>regular</strong> or <strong>strictly singular</strong>.</p> <ul> <li> <strong>Linear Regression:</strong> Regular model.</li> <li> <strong>FFNN:</strong> Strictly singular.</li> </ul> <hr> <h2 id="the-prior-and-singularity">The Prior and Singularity</h2> <h3 id="3-prior">3. Prior</h3> <p>$\varphi(w)$ is a “subjective” distribution of parameters $w$ based on the experimenter’s prior beliefs.</p> <ul> <li> <strong>Example:</strong> $\varphi(w) = N(0,1)$</li> </ul> <p>In Bayesian statistics, we do not want to learn the true weights, but the <strong>true distribution</strong> of the weights. Generally, as the number of training samples $n \to \infty$, we do not care about the prior (subject to some restrictions).</p> <p><strong>Theorem:</strong> FFNN under the regression models are <strong>strictly singular</strong>; that is, the Fisher Information Matrix $I(w)_{j,u}$ is degenerate: \(I(w)_{j,u} = \int_{\mathbb{R}^N} \left\langle \frac{\partial f(x,w)}{\partial w_j}, \frac{\partial f(x,w)}{\partial w_u} \right\rangle q(x) dx\)</p> <hr> <h2 id="bayesian-posterior-and-free-energy">Bayesian Posterior and Free Energy</h2> <p>The Bayesian posterior is given by: \(p(w|D_n) = \frac{p(D_n|w)\varphi(w)}{P(D_n)}\)</p> <table> <tbody> <tr> <td>Where $p(D_n</td> <td>w) = \prod_{i=1}^n P(y_i</td> <td>x_i, w)$. Since $P(D_n)$ does not depend on $w$, it serves as a normalizing constant:</td> </tr> <tr> <td>$$P(D_n) = \int_W p(D_n</td> <td>w)\varphi(w) dw$$</td> <td> </td> </tr> </tbody> </table> <h3 id="definitions">Definitions</h3> <p>The posterior can be expressed via the average log-loss $L_n(w)$: \(P(w|D_n) = \frac{1}{Z_n} \varphi(w) e^{-n L_n(w)}\) \(L_n(w) = -\log P(D_n|w) = \frac{1}{n} \sum_{i=1}^n \frac{1}{2} \| y_i - f(x_i, w) \|^2 + \text{const. (MSE)}\)</p> <p>$Z_n = \int_W e^{-n L_n(w)} \varphi(w) dw$ is the <strong>partition function</strong> (from statistical physics).</p> <p><strong>Free Energy:</strong> The free energy of a compact $W \subseteq W$ is: \(F_n(W) = -\log Z_n(W) = -\log \left( \int_W e^{-n L_n(w)} \varphi(w) dw \right)\)</p> <hr> <h2 id="neural-networks-posterior-using-mcmc">Neural Networks Posterior using MCMC</h2> <p>We use MCMC or HMC algorithms for sampling from the posterior $P(w|D_n)$. We expect samples to concentrate in regions of:</p> <ul> <li> <strong>High posterior density</strong> $\implies$ Low free energy $\implies$ Low generalization error $\implies$ “Good models” (low error, low complexity).</li> </ul> <h3 id="example-model-two-layer-ffnn">Example Model: Two-layer FFNN</h3> <p>2 nodes, 2 inputs, 1 output: \(f(x,w) = q_1 \cdot \text{ReLU}(\langle w_1, x \rangle + b_1) + q_2 \cdot \text{ReLU}(\langle w_2, x \rangle + b_2) + c\) $w_i \in \mathbb{R}^2, q_i, b_i, c \in \mathbb{R}$.</p> <table> <tbody> <tr> <td> <strong>Truth:</strong> If the truth is realizable (e.g., deforming from 2 nodes $\to$ 1 node), then the set of true parameters $w_0 = { w \mid P(y</td> <td>x,w) = q(y</td> <td>x) }$ is non-empty.</td> </tr> </tbody> </table> <h3 id="reading-the-plots--singularity">Reading the Plots &amp; Singularity</h3> <ol> <li> <table> <tbody> <tr> <td> <strong>Scaling Symmetry:</strong> $w_i$ normalized $\to \hat{w}_i =</td> <td>q_i</td> <td>w_i$.</td> </tr> </tbody> </table> </li> <li> <strong>Permutation Symmetry:</strong> $\hat{w}_1$ and $\hat{w}_2$ are superimposed.</li> </ol> <p>These symmetries are essentially what give Neural Networks their <strong>singularity</strong>.</p> <hr> <h2 id="the-free-energy-formula">The Free Energy Formula</h2> <p>Neural networks need not always prefer weights that minimize the loss; rather, they prefer those minimizing <strong>free energy</strong>.</p> <p><strong>Free Energy Formula:</strong> \(F_n = \underbrace{n L_n(w_0)}_{\text{Energy}} + \underbrace{\lambda \log n}_{\text{Entropy}}\)</p> <ul> <li> <table> <tbody> <tr> <td>$w_0 = { w \in W \mid p(x</td> <td>w) = q(x) }$ which is equivalent to $K(q(x) | p(x</td> <td>w)) = 0$.</td> </tr> </tbody> </table> </li> <li>Average log loss: $L(w) = S + K(q | p)$, where $S$ is Entropy.</li> </ul> <h3 id="loss-landscape-visualization">Loss Landscape Visualization</h3> <p>For realizable models, $w_0 = w_{opt}$.</p> <pre><code class="language-mermaid">graph TD
    subgraph Loss_Landscape [Loss Landscape L(w)]
    A[Local Minimum u_0] --- B[Global Minimum w_0]
    B --- S_Line[Entropy Level S]
    end
</code></pre> <p><em>(In this visualization, $w_0$ represents the true parameters where $L=S$. A model might get “stuck” at the local minimum $u_0$, but $w_0$ is preferred by the free energy for realizable models.)</em></p> <script type="module">
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
  mermaid.initialize({ startOnLoad: true });
</script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async="" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/welcome/">Welcome to my universe</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Agastya Agrawal. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: February 23, 2026. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>