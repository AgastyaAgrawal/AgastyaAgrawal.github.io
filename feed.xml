<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://agastyaagrawal.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://agastyaagrawal.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-23T23:31:02+00:00</updated><id>https://agastyaagrawal.github.io/feed.xml</id><title type="html">blank</title><subtitle>I am currently a second year undergraduate at Chennai Mathematical Institute pursuing my Bachelors in Mathematics and Computer Science. </subtitle><entry><title type="html">Mathematical Introduction to Neural Networks</title><link href="https://agastyaagrawal.github.io/blog/2026/mathintronn/" rel="alternate" type="text/html" title="Mathematical Introduction to Neural Networks"/><published>2026-02-10T00:00:00+00:00</published><updated>2026-02-10T00:00:00+00:00</updated><id>https://agastyaagrawal.github.io/blog/2026/mathintronn</id><content type="html" xml:base="https://agastyaagrawal.github.io/blog/2026/mathintronn/"><![CDATA[<h1 id="neural-networks-and-the-bayesian-posterior">Neural Networks and the Bayesian Posterior</h1> <p><strong>Aim:</strong> Using a concrete example of Neural Networks (NNs), introduce the posterior and the free energy, thus explaining the intuition of:</p> \[F_n \approx n \min_{\omega \in W} L_n(\omega) + \lambda \log n\] <blockquote> <p><strong>Key Insight:</strong> In singular models, non-true parameters can nonetheless be preferred by the posterior.</p> </blockquote> <hr/> <h2 id="feedforward-relu-neural-networks">Feedforward ReLU Neural Networks</h2> <p><strong>Definition:</strong> A feedforward ReLU Neural Net function is a function: \(f : \mathbb{R}^N \times W \longrightarrow \mathbb{R}^H\)</p> <ul> <li>$N$: number of inputs (in statistical models)</li> <li>$W$: parameter space</li> <li>$H$: number of outputs</li> </ul> <h3 id="architecture-diagram">Architecture Diagram</h3> <pre><code class="language-mermaid">graph LR
    subgraph Inputs
    x1((x1))
    dots1[...]
    xn((xn))
    end
    
    subgraph HiddenLayer1 [Hidden Layer 1]
    h1_1(( ))
    h1_2(( ))
    dots2[...]
    h1_n(( ))
    end

    subgraph HiddenLayerL [Hidden Layer L-1]
    hL_1(( ))
    hL_2(( ))
    end

    subgraph Output
    y1((y1))
    dots3[...]
    yn((yn))
    end

    x1 --- h1_1
    x1 --- h1_2
    xn --- h1_1
    xn --- h1_n
    h1_1 --- hL_1
    h1_n --- hL_2
    hL_1 --- y1
    hL_2 --- yn
    hL_2 --- y1
</code></pre> <p>This induces a function where both $x$ and $w$ are vectors: \(f(x, w) = (A^L \circ \text{ReLU} \circ A^{L-1} \circ \text{ReLU} \dots \circ A^1)(x)\)</p> <p>Each $A^L$ is parameterized by weights $W^L$ and biases $b^L$: \(A^L(z) = (w^L)^T z + b^L\)</p> <p><em>Note: You do not evaluate with ReLU on the last layer in regression.</em></p> <hr/> <h2 id="regression-with-neural-networks">Regression with Neural Networks</h2> <table> <tbody> <tr> <td>We define the triplet $(\text{model, truth, prior}) = (P(y</td> <td>x,w), q(y</td> <td>x), \varphi(w))$.</td> </tr> </tbody> </table> <h3 id="1-truth">1. Truth</h3> <p>We suppose i.i.d. data $d_n = { (X_1, Y_1), \dots, (X_n, Y_n) }$ which is drawn from an unknown true distribution $q(y|x)$.</p> <h3 id="2-model">2. Model</h3> <p>Given a FFNN function $f(x,w)$, the regression model is given by: \(p(y|x,w) = f(x,w) + \varepsilon, \quad \varepsilon \sim N(0,1) \text{ (noise)}\) \(p(y|x,w) = \exp \left( -\frac{1}{2} \| y - f(x,w) \|^2_W \right)\)</p> <p>It is the property of $f(x,w)$ that determines whether the model is <strong>regular</strong> or <strong>strictly singular</strong>.</p> <ul> <li><strong>Linear Regression:</strong> Regular model.</li> <li><strong>FFNN:</strong> Strictly singular.</li> </ul> <hr/> <h2 id="the-prior-and-singularity">The Prior and Singularity</h2> <h3 id="3-prior">3. Prior</h3> <p>$\varphi(w)$ is a “subjective” distribution of parameters $w$ based on the experimenter’s prior beliefs.</p> <ul> <li><strong>Example:</strong> $\varphi(w) = N(0,1)$</li> </ul> <p>In Bayesian statistics, we do not want to learn the true weights, but the <strong>true distribution</strong> of the weights. Generally, as the number of training samples $n \to \infty$, we do not care about the prior (subject to some restrictions).</p> <p><strong>Theorem:</strong> FFNN under the regression models are <strong>strictly singular</strong>; that is, the Fisher Information Matrix $I(w)_{j,u}$ is degenerate: \(I(w)_{j,u} = \int_{\mathbb{R}^N} \left\langle \frac{\partial f(x,w)}{\partial w_j}, \frac{\partial f(x,w)}{\partial w_u} \right\rangle q(x) dx\)</p> <hr/> <h2 id="bayesian-posterior-and-free-energy">Bayesian Posterior and Free Energy</h2> <p>The Bayesian posterior is given by: \(p(w|D_n) = \frac{p(D_n|w)\varphi(w)}{P(D_n)}\)</p> <table> <tbody> <tr> <td>Where $p(D_n</td> <td>w) = \prod_{i=1}^n P(y_i</td> <td>x_i, w)$. Since $P(D_n)$ does not depend on $w$, it serves as a normalizing constant:</td> </tr> <tr> <td>$$P(D_n) = \int_W p(D_n</td> <td>w)\varphi(w) dw$$</td> <td> </td> </tr> </tbody> </table> <h3 id="definitions">Definitions</h3> <p>The posterior can be expressed via the average log-loss $L_n(w)$: \(P(w|D_n) = \frac{1}{Z_n} \varphi(w) e^{-n L_n(w)}\) \(L_n(w) = -\log P(D_n|w) = \frac{1}{n} \sum_{i=1}^n \frac{1}{2} \| y_i - f(x_i, w) \|^2 + \text{const. (MSE)}\)</p> <p>$Z_n = \int_W e^{-n L_n(w)} \varphi(w) dw$ is the <strong>partition function</strong> (from statistical physics).</p> <p><strong>Free Energy:</strong> The free energy of a compact $W \subseteq W$ is: \(F_n(W) = -\log Z_n(W) = -\log \left( \int_W e^{-n L_n(w)} \varphi(w) dw \right)\)</p> <hr/> <h2 id="neural-networks-posterior-using-mcmc">Neural Networks Posterior using MCMC</h2> <p>We use MCMC or HMC algorithms for sampling from the posterior $P(w|D_n)$. We expect samples to concentrate in regions of:</p> <ul> <li><strong>High posterior density</strong> $\implies$ Low free energy $\implies$ Low generalization error $\implies$ “Good models” (low error, low complexity).</li> </ul> <h3 id="example-model-two-layer-ffnn">Example Model: Two-layer FFNN</h3> <p>2 nodes, 2 inputs, 1 output: \(f(x,w) = q_1 \cdot \text{ReLU}(\langle w_1, x \rangle + b_1) + q_2 \cdot \text{ReLU}(\langle w_2, x \rangle + b_2) + c\) $w_i \in \mathbb{R}^2, q_i, b_i, c \in \mathbb{R}$.</p> <table> <tbody> <tr> <td><strong>Truth:</strong> If the truth is realizable (e.g., deforming from 2 nodes $\to$ 1 node), then the set of true parameters $w_0 = { w \mid P(y</td> <td>x,w) = q(y</td> <td>x) }$ is non-empty.</td> </tr> </tbody> </table> <h3 id="reading-the-plots--singularity">Reading the Plots &amp; Singularity</h3> <ol> <li> <table> <tbody> <tr> <td><strong>Scaling Symmetry:</strong> $w_i$ normalized $\to \hat{w}_i =</td> <td>q_i</td> <td>w_i$.</td> </tr> </tbody> </table> </li> <li><strong>Permutation Symmetry:</strong> $\hat{w}_1$ and $\hat{w}_2$ are superimposed.</li> </ol> <p>These symmetries are essentially what give Neural Networks their <strong>singularity</strong>.</p> <hr/> <h2 id="the-free-energy-formula">The Free Energy Formula</h2> <p>Neural networks need not always prefer weights that minimize the loss; rather, they prefer those minimizing <strong>free energy</strong>.</p> <p><strong>Free Energy Formula:</strong> \(F_n = \underbrace{n L_n(w_0)}_{\text{Energy}} + \underbrace{\lambda \log n}_{\text{Entropy}}\)</p> <ul> <li> <table> <tbody> <tr> <td>$w_0 = { w \in W \mid p(x</td> <td>w) = q(x) }$ which is equivalent to $K(q(x) | p(x</td> <td>w)) = 0$.</td> </tr> </tbody> </table> </li> <li>Average log loss: $L(w) = S + K(q | p)$, where $S$ is Entropy.</li> </ul> <h3 id="loss-landscape-visualization">Loss Landscape Visualization</h3> <p>For realizable models, $w_0 = w_{opt}$.</p> <pre><code class="language-mermaid">graph TD
    subgraph Loss_Landscape [Loss Landscape L(w)]
    A[Local Minimum u_0] --- B[Global Minimum w_0]
    B --- S_Line[Entropy Level S]
    end
</code></pre> <p><em>(In this visualization, $w_0$ represents the true parameters where $L=S$. A model might get “stuck” at the local minimum $u_0$, but $w_0$ is preferred by the free energy for realizable models.)</em></p> <script type="module">
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
  mermaid.initialize({ startOnLoad: true });
</script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async="" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>]]></content><author><name></name></author><category term="Machine Learning"/><category term="AI Alignment"/><category term="SLT"/><summary type="html"><![CDATA[Neural Networks and the Bayesian Posterior]]></summary></entry><entry><title type="html">Welcome to my universe</title><link href="https://agastyaagrawal.github.io/blog/2026/welcome/" rel="alternate" type="text/html" title="Welcome to my universe"/><published>2026-01-01T10:00:00+00:00</published><updated>2026-01-01T10:00:00+00:00</updated><id>https://agastyaagrawal.github.io/blog/2026/welcome</id><content type="html" xml:base="https://agastyaagrawal.github.io/blog/2026/welcome/"><![CDATA[<p>Welcome to my blog!</p> <p>I will be using this space to document my notes, and in general post any academic topic that I find super exciting! If you would like me to write about something you have trouble understanding or would in general want me to write about some topic, feel free to mail me!</p> <p>Best regards, Agastya</p>]]></content><author><name></name></author><category term="general"/><category term="updates"/><summary type="html"><![CDATA[A quick introduction to my new blog.]]></summary></entry></feed>